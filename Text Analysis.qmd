---
title: "Gotham Culture Division of Sport Fish Staff comments Text Analysis"
author: "Adam Reimer"
format: html
editor: visual
---

## Introduction

Gotham Culture provided A Division of Sport Fish specific report and 31 pages of staff comments to Division Leadership in late April. The staff comments were enlightening but also contained some confidential information. This text analysis of those comments represents an effort to share the information contained in the comments broadly while respecting staff privacy.

```{r}
#| echo: false
#| output: false
packs <- c("tidyverse", "stringr", "tidytext", "textdata", "wordcloud")
lapply(packs, require, character.only = TRUE)
```

The complete question, the abbreviated question we used in subsequent figure and the number of responses associated with each question are shown in Table 1.

```{r}
#| echo: false
#| label: tbl-questions
#| tbl-cap: Questions asked and number of responses received, DSF employees. 
input <- 
  xlsx::read.xlsx(".\\Copy of Sport Fish survey comments_sg.xlsx", 1, as.data.frame = TRUE, header = FALSE) 

# format questions
questions <- 
  input[grepl("^Q\\d.*", input$X1), ] %>%
  sapply(function(x){gsub("\n", " ", x)}) %>%
  setNames(NULL)
q_short <- c("Q21: enhance safety/wellbeing", "Q31: increase retention", "Q35: positive job satisfaction factors", 
             "Q36: negative job satisfaction factors", 
             "Q40: reasons not to report", "Q41: disrespect impact", "Q48: biases oberved", "Q49: bias impact",
             "Q54: schedule, workload, work/life integration")
q_index <- c(which(grepl("^Q\\d.*", input$X1)), length(input$X1) + 1) #add one so that last row has a question
q_vector <- rep(questions, q_index[2:length(q_index)] - q_index[1:(length(q_index) - 1)])

gc <- 
  input %>%
  mutate(question = factor(q_vector, levels = questions, labels = q_short))%>%
  filter(!grepl("^Q\\d.*", X1)) %>%
  tibble(response = seq_along(X1), text = X1) %>% 
  select(-X1) 

gc_word <- 
  gc %>%
  unnest_tokens(word, text)
  
gc_biword <- 
  gc %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

gc %>%
  group_by(question) %>%
  summarise(n = n()) %>%
  mutate(question_long = questions) %>%
  select(question_long, question, n) %>%
  knitr::kable()
```

One way to quantify staff comments is to tally the most frequently used words within each question.

```{r}
#| echo: false
#| label: fig-words
#| fig-cap: Most common word associated with each survey question, DSF employees.
gc_topwords <-
  gc_word %>%
  anti_join(stop_words) %>% #stop words is a built in data set of the most common words
  group_by(question) %>%
  count(word) %>%
  top_n(11) #11 so we can drop one for the stop criteria

gc_topwords %>%
  group_by(question) %>%
  mutate(stop = min(median(n), min(n))) %>%
  filter(n > stop) %>%
  ggplot(aes(x = n, y = reorder_within(word, n, question), fill = question)) +
    geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
    facet_wrap(~ question, ncol = 2, scales = "free_y") +
    xlab("Number of Occurrences") +
    ylab("Most Common Words") +
    scale_y_discrete(labels = function(x) gsub("__.+$", "", x))
```
